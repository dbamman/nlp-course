{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689cbcbc-f951-4fd1-95ee-ff2bac2f14b7",
   "metadata": {},
   "source": [
    "# INFO 159/259"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484bdfab-9192-4359-a390-63f40c70029a",
   "metadata": {},
   "source": [
    "# <center> Homework 3: Transformers and Masked Language Models </center>\n",
    "<center> Due: <b>Monday</b>, February 23, 2026 @ 11:59pm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caee50a-3b0b-4286-aea5-9ffd7e4aaf91",
   "metadata": {},
   "source": [
    "In this homework, you will experiment with a BERT-style bi-directional encoder transformer model that has been pretrained on the masked language modeling task.\n",
    "\n",
    "In the first part, you will extract the _contextual_ embedding representations of words in the context of their sentences.\n",
    "\n",
    "In the second part, you will explore how the embedding representation changes at different layers of the model.\n",
    "\n",
    "\n",
    "Learning objectives:\n",
    "- Understand the masked language modeling objective\n",
    "- Be able to explain the difference between contextual and static representations\n",
    "- Understand the transformers architecture\n",
    "\n",
    "During the course of this project, you may need to inspect intermediate outputs to understand how they are structured (e.g., what exactly is in the output of the BERT model?) Feel free to create extra cells to do this investigation.\n",
    "\n",
    "You should also be using a GPU instance on Google Colab to run your code. You can enable a GPU kernel by clicking `Runtime > Change runtime type` and selecting `T4 GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2cbc8-3234-4658-809c-f871f53b47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/promotion.n.xml -O promotion.n.xml\n",
    "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/hw3_transformers/hw3_utils.py -O hw3_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32a029-b45f-4f11-b6a4-b329f3ce8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "\n",
    "from hw3_utils import load_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee5813-052b-4763-9828-de92662fa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf29d1-6e64-498a-8145-a225400d35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = load_data_file(\"promotion.n.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24cf5a-1325-4b21-9b93-1eeb18060f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may get a warning in the LOAD REPORT. This is ok!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde5e12-4a21-4682-8d8f-34b29cb9d6b3",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "We will begin by implementing the `tokenize` and `collate` functions. These two functions, together, will prepare your data to be fed into the BertModel (see the [huggingface documentation here](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward).)\n",
    "\n",
    "The `tokenize` function converts a sentence into the token IDs that are recognizable to the model; it also generates an attention mask. It takes an input batch (a dictionary of lists) and returns an output batch (also a dictionary of lists).\n",
    "\n",
    "```\n",
    "Input:\n",
    "{\n",
    "    \"word\": list[str]\n",
    "    \"sentence\": list[list[str]]\n",
    "}\n",
    "\n",
    "Output:\n",
    "{\n",
    "    # These are generated by calling tokenizer()\n",
    "    \"input_ids\": list[list[int]]\n",
    "    \"token_type_ids\": list[list[int]]\n",
    "    \"attention_mask\": list[list[int]]\n",
    "\n",
    "    # Write the code to find the token indices\n",
    "    \"token_indices\": list[int]\n",
    "}\n",
    "```\n",
    "\n",
    "You will want to store the index of the first subword token that corresponds to the target word in `batch[\"word\"]`. Here is an example:\n",
    "\n",
    "```\n",
    "Text:            I    said    hello    world    .\n",
    "\n",
    "Tokens: [CLS]    I    said    hello    world    .    [SEP]\n",
    "IDs:     101   1045   2056    7592     2088    1012   102\n",
    "Index:    0      1     2        3        4      5      6\n",
    "                                ^\n",
    "```\n",
    "The token `hello` has token index 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f5d27-97f7-4a36-a5af-670e6bb3f7b3",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    # TODO: Implement me!\n",
    "    ...\n",
    "    \n",
    "    assert not any(x is None for x in output[\"token_indices\"]), \"Target token not found in sentence!\"\n",
    "    assert len(token_indices) == len(batch[\"sentence\"]), \"Token indices is the wrong length!\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010bd65-a167-4417-bfb2-c0ae4fbd8918",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Quick check**: This should be the output of the below cell. Your decoded token should also match the target word.\n",
    "```\n",
    "{'input_ids': [[101, 1045, 2056, 7592, 2088, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]], 'token_indices': [4]}\n",
    "\n",
    "Selected token ID:  2088\n",
    "Decoded token:  world\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfecf5-fd95-4714-87dd-ba6a6ef16d0a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    tokens = tokenize({\n",
    "        \"word\": [\"world\"],\n",
    "        \"sentence\": [[\"I\", \"said\", \"hello\", \"world\", \".\"]]\n",
    "    })\n",
    "    print(tokens)\n",
    "    print()\n",
    "\n",
    "    print(\"Selected token ID: \", tokens[\"input_ids\"][0][tokens[\"token_indices\"][0]])\n",
    "    print(\"Decoded token: \", tokenizer.decode(tokens[\"input_ids\"][0][tokens[\"token_indices\"][0]]))\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a151f-310f-4d6b-ac0f-375217b044b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The `collate` function converts a list of rows into a batched tensor that the model can process in parallel. It takes a list of rows (a list of dicts) and returns a dictionary of tensors that can be fed into the model. It should use the `tokenize` function you implemented.\n",
    "\n",
    "```\n",
    "Input:\n",
    "[{\"word\": str, \"sentence\": str}, ...]\n",
    "\n",
    "Output:\n",
    "{\n",
    "    \"input_ids\": torch.tensor\n",
    "    \"token_type_ids\": torch.tensor\n",
    "    \"attention_mask\": torch.tensor\n",
    "    \"token_indices\": torch.tensor\n",
    "}\n",
    "```\n",
    "\n",
    "Since sentences might be different lengths, you will want to `pad` the sequences before converting to torch tensors. You might want to look into `torch.nn.utils.rnn.pad_sequence`.\n",
    "\n",
    "Each of `input_ids`, `token_type_ids`, and `attention_mask` should have shape `(B, L)`, where `B` is the batch size and `L` is the maximum sequence length in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425e1f6-ae0b-4a21-9053-caf769264755",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(items, device=\"cpu\"):\n",
    "    # TODO: Implement me!\n",
    "    ...\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c65237-5e1a-4401-923b-3bb7f31c6d80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Quick check**: This should be the output of the following cell.\n",
    "\n",
    "```\n",
    "{'input_ids': tensor(\n",
    "    [[  101, 18558, 18914,  2003,  2019, 17953,  2361,  2607,  1012,   102],\n",
    "     [  101,  1045,  2293, 17953,  2361,   999,   102,     0,     0,     0],\n",
    "     [  101,  2054,  2515, 17953,  2361,  3233,  2005,  1029,  1029,   102]]),\n",
    " 'token_type_ids': tensor(\n",
    "    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
    " 'attention_mask': tensor(\n",
    "    [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "     [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
    " 'token_indices': tensor([5, 3, 3])}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b940eb-9519-482e-a4d7-8ff087aaeb91",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def _():\n",
    "    return collate([\n",
    "        dict(word=\"NLP\", sentence=\"INFO 159 is an NLP course .\".split()),\n",
    "        dict(word=\"NLP\", sentence=\"I love NLP !\".split()),\n",
    "        dict(word=\"NLP\", sentence=\"What does NLP stand for ? ?\".split()),\n",
    "    ])\n",
    "_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24119d-9a17-455c-aeb9-650e900eedf6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Answer these questions.** (You may execute arbitrary code if necessary.)\n",
    "1. Why are there more `input_ids` in each sequence compared to the number of (space-delimited) tokens?\n",
    "2. List all of the extra (special) tokens that get added to the input. Write the decoded tokens (not the integer input IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fec1b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f848a11-1f86-47c5-be13-c2e45a3cf80f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Extracting contextual embedding\n",
    "\n",
    "Here, you will implement `get_token_embedding` to get the contextual embedding. For the $i$th sentence in the batch, you want to extract the embedding representation for the $i$th token index in `token_indices` at the specified layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c71908-5af4-478c-a05c-52e74a3a8067",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_token_embedding(model_output, token_indices, layer=-1):\n",
    "    # batch_reps should have shape (B, D)\n",
    "    # where B is the batch size and D is the dimension of the hidden state\n",
    "    # (for BERT, D = 768)\n",
    "\n",
    "    # TODO: Implement me!\n",
    "    ...\n",
    "    return batch_reps.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712aaa3-c4d6-4132-bb54-4fb72ad3e8a7",
   "metadata": {},
   "source": [
    "We implement the inference code for you, but read through it and make sure you understand what is going on! Calling `model(...)` calls the `.forward()` method of the model as well as the necessary pre- and post-processing steps (see the [HF documentation](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward) for the BertModel, and the note at the bottom about overriding the `__call__` method).\n",
    "\n",
    "Setting up the code this way (with `iter_outputs` serving as a generator that yields model outputs) lets us easily iterate through model outputs and apply arbitrary functions to them (like our `get_token_embedding` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b326a-d7cb-421a-8eac-e191f677b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def iter_outputs(data, model, batch_size=128):\n",
    "    model.eval()  # setting eval mode disables dropout (and other stuff)\n",
    "    model.to(device)  # we put the model on the GPU\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate  # we pass in our collate function here\n",
    "    )\n",
    "\n",
    "    # disable gradient calculation and storage for efficiency,\n",
    "    # since we aren't backpropagating\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            output = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                token_type_ids=batch[\"token_type_ids\"].to(device),\n",
    "                # by default, this only returns the last layer hidden states\n",
    "                # we want the flexibility to look at other layers, so we set\n",
    "                # `output_hidden_states=True`\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            yield batch, output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a224f27-c081-4ead-9c72-b192268ad4b3",
   "metadata": {},
   "source": [
    "**Quick check**: you should get the following output\n",
    "```\n",
    "tensor([[ 0.5100, -1.1154,  0.6605,  ..., -0.2650,  1.5101,  0.5656],\n",
    "        [ 0.9360, -1.9150,  0.8224,  ...,  0.1231,  0.9546, -0.1965]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3588e-057f-4281-8432-c620fc7e3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    embeddings = []\n",
    "    for batch, batch_output in iter_outputs([dict(word=\"NLP\", sentence=\"INFO 159 is an NLP course .\".split())], model):\n",
    "        embeddings.append(get_token_embedding(batch_output, batch[\"token_indices\"]))\n",
    "        embeddings.append(get_token_embedding(batch_output, batch[\"token_indices\"], layer=4))\n",
    "    embeddings = torch.concat(embeddings, dim = 0)\n",
    "    return embeddings\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a345261-ea7d-4f86-9586-685e8ae16b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings():\n",
    "    embeddings = []\n",
    "    for batch, batch_output in iter_outputs(items, model):\n",
    "        embeddings.append(get_token_embedding(batch_output, batch[\"token_indices\"]))\n",
    "    \n",
    "    return torch.concat(embeddings, dim=0)\n",
    "\n",
    "embeddings = get_all_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb88c4a-e363-4078-b67a-7e3d15d510c9",
   "metadata": {},
   "source": [
    "## Exploring contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26948cdc-f23e-427b-ae74-2d9d46620cc1",
   "metadata": {},
   "source": [
    "With contextual embeddings, the representations change depending on the context; let's see that in action by looking at the sentences where the target word embeddings have the greatest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744ce21-572a-4c83-8866-07566bca1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(vec, matrix, k=10):\n",
    "    cos_sim = ((vec @ matrix.T) / (vec.norm() * matrix.T.norm(dim=0, keepdim=True))).squeeze()\n",
    "    inds = torch.argsort(-cos_sim)[:k]\n",
    "    return inds, cos_sim[inds]\n",
    "\n",
    "def show_nearest_neighbor_sentences(index, embeddings):\n",
    "    print(f\"QUERY (target word={items[index]['word']}):\")\n",
    "    print(\" \".join(items[index][\"sentence\"]))\n",
    "\n",
    "    print(\"NEIGHBORS:\")\n",
    "    inds, _ = nearest_neighbors(embeddings[index], embeddings, k=5)\n",
    "    for ind in inds:\n",
    "        print(\"-\", \" \".join(items[ind][\"sentence\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99a961-ecb9-479f-a289-f3b26c9a60e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Answer the following question**\n",
    "1. Identify at least two different contexts in which the target word `promotion` appears. Include the query sentence, one nearest neighbor (not the query), and the ID of the query sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f4dd2",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd536894-ea3c-4cae-871d-e28742574651",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "show_nearest_neighbor_sentences(2, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25625f5d-4d32-4603-8034-84b37c6bebea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Attention masking\n",
    "\n",
    "Let's take a closer look at how attention masks affect the output of the masked language model.\n",
    "\n",
    "Consider embedding these two sentences which only differ in one token. We modify the attention mask to ignore this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d1669-2e29-4c26-84d8-c7dac52af01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_items = [\n",
    "    dict(word=\"cat\", sentence=\"Have you fed your cat yet ?\".split()),\n",
    "    dict(word=\"dog\", sentence=\"Have you fed your dog yet ?\".split())\n",
    "]\n",
    "\n",
    "model_inputs = collate(custom_items, device=device)\n",
    "# ignore the tokens located at token_indices for each data point\n",
    "model_inputs[\"attention_mask\"][[0, 1], model_inputs[\"token_indices\"]] = 0\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007828c-6fd8-49fd-8030-a3aaae9b50d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Answer this question**: (you can add cells and run code to arrive at the answer)\n",
    "\n",
    "If we were to put these inputs through the BERT model, which of the following would be true? For each, explain why or why not.\n",
    "\n",
    "1. The last layer hidden representation of the target tokens (`cat` and `dog`) would be the same.\n",
    "2. The last layer hidden representation of the third non-special token (`fed` in both cases) would be the same.\n",
    "3. The first layer hidden representation and the last layer hidden representation of each target token (e.g., the first and last layer representation of the `cat` token) would be the same.\n",
    "4. If we mask out everything _but_ the target tokens, the first and last layer hidden representations of each target token would be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db7478",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3314fe-0a75-4c6f-8606-ffa74b8912b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Upload instructions\n",
    "\n",
    "Upload your `.ipynb` file (with all of the cells executed so that the outputs are visible) to Gradescope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
